\documentclass[12pt, letterpaper]{article}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{parskip}

\usepackage{graphicx}
\graphicspath{{../images/}}

\usepackage{hyperref}
\hypersetup{
    colorlinks=true,
    urlcolor=blue,
    linkcolor=blue,
    citecolor=blue,
    filecolor=blue,
}
\urlstyle{same}

\title{Paper Structure}
\author{}
\date{}


\begin{document}

\maketitle
\tableofcontents
\setcounter{tocdepth}{3}

\section{Introduction} % 1 Page

% ---------------------------------------------------------------------------- %

\section{Methods} % Total of 9 + 8 = 17 Pages

% ---------------------------------------------------------------------------- %

\subsection{Preprocessing} % Total of 6 + 3 = 9 Pages

\subsubsection{Feature Engineering} % Total of 6 Pages

\textbf{Images} % 3-4 Pages

\textbf{Reviews} % 2 Pages
%%%
\begin{itemize}
    \item Description of Sentiment Analysis, stating procedure and results and including \textbf{Figure} with Wordcloud, either only English Words or Side-by-Side Wordclouds of English and Norwegian Words
    \item In addition: Language Detection to include the \emph{number of different languages} and the \emph{fraction of norwegian languages} and Analyzing the reviews lengths to include the \emph{median review length}
    \item Since there are multiple reviews per apartment the results for each review were averaged for each apartment separately.
\end{itemize}
%%%

\textbf{Others} % 0-1 pages
%%%
\begin{itemize}
    \item Optionally mention all other features that we added to the dataset
    \item All self-engineered features from images, from reviews and from existing metric variables were combined into a single dataframe as the foundation of all further analysis
\end{itemize}
%%%


\subsubsection{Feature Selection} % 3 Pages

% ---------------------------------------------------------------------------- %

\subsection{Models} % Total of 2 + 5 + 1 = 8 Pages

\subsubsection{Classical Models} % 2 Pages
%%%
\begin{itemize}
    \item serve as benchmark models to better evaluate performance of custom neural network
    \item selected with increasing degrees of complexity and corresponding decreasing degree of interpretability
    \item Focus on $4$ models: \texttt{LinearRegression}, \texttt{Ridge}, \texttt{RandomForest} and \texttt{HistGradientBoosting}
    \item Describe Model Fitting process and hyperparameter tuning with Randomized Search Cross Validation
\end{itemize}
%%%

\subsubsection{Neural Network} % 5 Pages


\subsubsection{Price Distribution} % 1 Page
%%%
\begin{itemize}
    \item \textbf{Figure} of Side-by-Side Histograms of Price and Log-Price Distribution
    \item Price Distribution right-skewed with some very large outliers
    \item Explain benefits of normally distributed dependent variable, particular for models with distributional assumptions (e.g. Linear Regression vs. Neural Network)
    \item State that all classical Machine Learning Models benefitted from log transformation
    \item Briefly discuss why we did not transform the price variable for the Neural Network
\end{itemize}
%%%

% ---------------------------------------------------------------------------- %

\section{Results} % Total of 4 + 4 = 8 Pages

% ---------------------------------------------------------------------------- %

\subsection{Predictive Performance} % 4 Pages
%%%
\begin{itemize}
    \item \textbf{Figure} of performance comparison between selected classical models and neural network for given feature selector (e.g. \texttt{RFE}) and different number of selected features
    \item Interpret Differences in Training and Validation Performance between different models
    \item Interpret Differences in Performance for different number of selected features
    \item Compare Performance on Validation Set with Performance on Test Set for the best model of each class by means of a table \\
          $\Rightarrow$ Models whose hyperparameters were tuned on validation set generalize worse to test set, e.g. \texttt{HistGradientBoosting}, \texttt{RandomForest} and \texttt{Ridge}
    \item Include average predictions of top 2/3/4/5 models, where models are selected based on validation set performance and Test Set predictions are averaged
    \item Potentially mention which models contributed to predictions on new, unseen dataset from challenge
\end{itemize}
%%%

% ---------------------------------------------------------------------------- %

\subsection{Explainability / Interpretability} % Total of 4 Pages

\subsubsection{Feature Importance} % 1-2 Pages
%%%
\begin{itemize}
    \item \textbf{Figure} of Coefficient Plot for Linear Regression with e.g. $25$ selected features
    \item Interpret Figure
\end{itemize}
%%%

\subsubsection{Sensitivity of Neural Network Performance on Outliers} % 2-3 Pages
%%%
\begin{itemize}
    \item State shortcomings of Neural Net to predict prices in the tails of the distribution, error metrics thus largely impacted by outliers
    \item State (maybe with a table) drastic increase in predictive performance when excluding largest quantiles of price distribution from dataset
    \item Discuss if the task itself is theoretically feasible for any kind of model
    \item \textbf{Figure} of latent space representation
    \item Discuss that the data is not expressive enough to capture all features that determine the price in reality, particularly for apartments with very high prices, that do not differ from lower priced apartments based on their feature set
    \item Question underlying assumptions that all apartments are reasonably priced, difficult to detect overpriced listings that bias model predictions
\end{itemize}
%%%

% ---------------------------------------------------------------------------- %

\section{Conclusion} % 1 Page

% ---------------------------------------------------------------------------- %

% ask for order of appendix and references
\section{Appendix}
%%%
\begin{itemize}
    \item include link to repository with codebase to reproduce all findings
\end{itemize}
%%%

% ---------------------------------------------------------------------------- %

\section{References}

\end{document} % Total of 1 + 17 + 8 + 1 = 27 Pages
