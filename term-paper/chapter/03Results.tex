\section{Results} \label{results}

\subsection{Evaluation Metrics}

As noted in \Cref{appendix:hyperparameters}, the Mean Squared Error Loss was used for \emph{training} the Neural Network due to its convenient differentiability with regards to backpropagation.
For \emph{evaluating} the different models, we preferred the Mean Absolute Error and the $R^2$ value.
The MAE measures absolute deviations between true and predicted price and is therefore interpretable on the same scale as the original data.
While the $R^2$ certainly has some inherent flaws, it provides a scale-independent and highly interpretable measure of overall model fit.

When using the \emph{log} price for model fitting, all error metrics were computed on the \emph{original} price scale to obtain an interpretable MAE value.
More specifically, all computations were performed \emph{after} backtransforming the fitted values for the log-price to the raw price scale.
Since the results for the MAE and $R^2$ are highly dependent on the exact transformation procedure, they are in particular \emph{not comparable} with groups that chose a different evaluation strategy.

To illustrate this difference in more detail, denote the error metrics as functions of the true and predicted values:
%%%
\begin{align*}
  MAE(\mathbf{y}, \hat{\mathbf{y}})
   & = \frac{1}{n}\sum_{i = 1}^{n} \left| y_i - \hat{y}_i \right|                                               \\
  R^2(\mathbf{y}, \hat{\mathbf{y}})
   & = \frac{\sum_{i=1}^{n} \left( \hat{y}_i - \bar{y}\right)^2}{\sum_{i=1}^{n} \left( y_i - \bar{y} \right)^2}
\end{align*}
%%%
With this notation, we computed $MAE \left(\mathbf{y}, \exp \left(\widehat{\log(\mathbf{y})}\right)\right)$ which is of course not identical to backtransforming the error on the log-scale $\exp \left(MAE \left(\log(\mathbf{y}), \widehat{\log(\mathbf{y})}\right) \right)$.
To stay consistent, we similarly computed $R^2 \left(\mathbf{y}, \exp \left(\widehat{\log(\mathbf{y})}\right)\right)$ on the original scale with the backtransformed fitted values instead of  $R^2 \left(\log(\mathbf{y}), \widehat{\log(\mathbf{y})}\right)$, where the latter usually leads to a better score.



\subsection{Predictive Performance}

%%%
\begin{figure}[t]
  \centering
  \includegraphics[width=\textwidth]{model_comparison.png}
  \caption{Performance Comparison of Classical Models with Neural Network}
  \label{fig:model-comparison}
\end{figure}
%%%

\Cref{fig:model-comparison} shows performance metrics for all fitted models on both, the training set and the validation set.
The different colors indicate how many features were selected by the \texttt{RFE} algorithm for this particular fit out of $59$ total predictor variables.
Unsurprisingly, one or two features (in case of the \texttt{RFE} the number of bedrooms and the (number of) accomodates) provide too little information to model the task appropriately.

However, including merely $5$ features (in this case adding the $30$ day availability, the indicator variable for the \emph{Frogner} neighbourhood and, notably, the price predictions from the Convolutional Net based on the image data) results in a very competitive performance for most models on the validation set.

The two subplots on the left displaying the MAE tell a very similar story to the subplots on the right that show the $R^2$:
Generally, more features lead to better performance on training and validation set.
In case of the flexible Histogram-based Gradient Boosting algorithm and, to some minor extent, for the Random Forest, a high number of input features lead to \emph{overfitting} such that the performance on the training set is far superior to the values on out-of-sample data.
In contrast, models with few parameters such as Linear Regression or Ridge Regression generalize very well to the validation set with virtually no performance drop.

One exception to this rule is the highly complex Neural Network which appears to perform \emph{better} on out-of-sample data.
This phenomenon can be explained to some extent by the different behaviour of \emph{Dropout} and \emph{Batchnorm} during training and inference

When comparing the models within each subplot, all classical Machine Learning models perform similarly on the validation data.
This finding emphasizes two aspects:
%%%
\begin{enumerate}
  \item The prediction task does \textbf{not} require overly complex models and linear models perform just fine.
  \item We can expect predictions within a distance of roughly $400$ NOK (40 Euros) on average to the true price.
        Further, a $R^2$ value of around $0.4$ is the best we can hope for.
        These statements hold for fitting on the \emph{entire} training set, \Cref{outliers} reveals how the performance radically improves when excluding some observations.
\end{enumerate}
%%%
In comparison to all \texttt{scikit-learn} models, our custom Neural Net appears to underperform.
We have to keep in mind though, that evaluation on the validation set is overly optimistic for those models whose hyperparameters were tuned on this data during cross validation.
The best simulation to performance on truly unseen data is thus provided by the \emph{test} set that was not used in any step up to this point.

\begin{table}[th]
  \centering
  \begin{tabular}{lrr}
    \hline
    Model                & \multicolumn{1}{c}{MAE} & \multicolumn{1}{c}{$R^2$} \\ \hline
    Linear Regression    & 404.709                 & 0.298                     \\
    Ridge                & 405.932                 & 0.294                     \\
    Random Forest        & 444.166                 & 0.268                     \\
    HistGradientBoosting & 412.243                 & 0.387                     \\
    Neural Network       & 402.24                  & 0.333                     \\
    Top2 Average         & 404.848                 & 0.296                     \\
    Top3 Average         & 399.315                 & 0.343                     \\
    Top4 Average         & 404.206                 & 0.332                     \\
    Top5 Average         & 408.116                 & 0.27                      \\ \hline
  \end{tabular}
  \caption{Test Set Performance of Classical Machine Learning Models, our custom Neural Network and Ensemble Predictions}
  \label{tab:test-set}
\end{table}

\Cref{tab:test-set} compares test set metrics for the best performing models on the validation set within each class (i.e. the Neural Net version with all $59$ features and the Random Forest model with only $25$ features were selected).
In addition, we added some simple ensemble models that predict the average estimate from the top $2$, $3$, $4$ or all five models, respectively.

The hyperparameter configuration of the Random Forest Model, that performed best for the validation data, generalizes worst to the test set.
Quite surpringly, the Neural Net shows a significant performance boost on the test data and is now very competitive with all other models.
One plausible explanation for this phenomenon is provided in \Cref{outliers}.

Averaging predictions from multiple algorithms indicates promising results:
The Top $3$ Average achieves the lowest out-of-sample MAE of slightly below $400$ NOK.




\subsection{Understanding and Interpretation}

Interpreting the results of high-dimensional and complex statistical models is notoriously difficult.
This section approaches the challenge from two opposite angles.

First, we reduce the \emph{complexity} by fitting a simple and well-understood Linear Regression model with the same features that were selected for the best-performing Neural Network and analyze the coefficients.

Second, we reduce the \emph{dimensionality} by leveraging a different Neural Network and visualize the results in two-dimensional space.

\subsubsection{Feature Importance}

In order to draw conclusions about which features were most important for the Neural Network, one could imagine to start with a random noise input combination and optimize for artificial inputs which maximally or minimally activate the output neuron, i.e. leading to very low or very high price predictions.
We chose a conceptually simpler approach and analyzed the feature importance of an auxiliary Linear Regression model instead to provide some suggestions that could potentially apply to the Neural Network as well.

Therefore, we preselected $25$ of the network's input features with the \texttt{RFE} algorithm, which is introduced in the \Cref{appendix:feature-selection}, and compared the coefficient magnitudes.
As noted before, all predictors were standardized such that a meaningful comparison is feasible.
The results are shown in \Cref{fig:coefficient-plot}.

\begin{figure}[t]
  \centering
  \includegraphics[width=0.8\textwidth]{coefficient_plot.png}
  \caption{Estimated coefficients of a Linear Regression model for $25$ preselected features}
  \label{fig:coefficient-plot}
\end{figure}

It is worth noting that the two most important features based on various different feature selectors of the \texttt{scikit-learn} library were always the number of \emph{bedrooms} and the (number of) \emph{accomodates} in this order, both measuring the apartment's \emph{capacity}.

The coefficient plot, however, is dominated by categorical features:
In agreement with human intuition, the \emph{room} type, the \emph{property} type and the \emph{neighbourhood} strongly influence the predicted price.
Unsurprisingly, the property types \emph{entire home} and \emph{entire villa} are connected with high prices, whereas the the room type \emph{shared room} correlates with cheaper apartments.
Notably, the price predictions from the Convolutional Network fitted on the image data indicates a significant positive effect, conditioned on all other selected features.

When analyzing the \emph{marginal} effect of neighbourhood on price by e.g. ordering the neighbourhoods according to their median price, this order is nearly identical to the ranking in \Cref{fig:coefficient-plot} with \emph{Frogner} at the top and \emph{Grorud} at the bottom.

Interestingly, apartments in the \emph{Sentrum} (central area) have a large positive coefficient in the regression, yet the second to lowest median price.
This finding indicates the presence of \emph{confounders}:
The city center might plausibly be connected to fewer rooms and smaller apartment sizes overall pulling the median price down.
These confounding effects are controlled for in the regression model, but not in the naive bivariate analysis.


\subsubsection{Sensitivity to Outliers} \label{outliers}

% Manually constructed with saved .csv file in tables folder and
% https://www.tablesgenerator.com/latex_tables
\begin{table}[t]
  \centering
  \begin{tabular}{@{}ccc@{}}
    \toprule
    Quantile Threshold & MAE    & $R^2$ \\ \midrule
    0.0                & 443.35 & 0.16  \\
    1.0                & 337.59 & 0.51  \\
    2.5                & 282.17 & 0.53  \\
    5.0                & 240.57 & 0.54  \\
    10.0               & 214.76 & 0.49  \\ \bottomrule
  \end{tabular}
  \caption{Mean Absolute Error and $R^2$ value of the Neural Network on the validation set after removing the highest quantiles of the price distribution from the data set}
  \label{tab:mlp-outliers}
\end{table}

\Cref{tab:mlp-outliers} shows performance metrics for the Neural Network fitted on a subset of the data in the validation set.
As indicated by the first column, the dataset was reduced by successively cutting off observations from the top of the price distribution.
More precisely, the first row refers to the entire data whereas the last row excludes the top $10$\% most expensive apartments.

By omitting just the top $1$\%, the MAE reduces by over $100$ NOK (about $10$ Euros) and the $R^2$ rapidly jumps to over $0.5$.
This sensitivity to outliers in the price distribution is \textbf{not} solved by log-transforming the price, all classical models from the previous chapter suffer from the same effect.

The values in \Cref{tab:mlp-outliers} also suggest an explanation why the Neural Network performs much better in the test set compared to the validation set, which can not be observed for all other models.
When partitioning the entire data into training, validation and test set, it just happened by chance that the test set contains fewer price outliers which drive the error metrics up compared to the validation set, even though the model is exactly the same.
In fact, when \emph{training} the Neural Network with some of the outliers removed, the training and validation metrics are much closer as well, such that we detected an additional factor besides \emph{Dropout} (see \Cref{dropout}) for the diverging behaviour.

In contrast, the classical Machine Learning models were fitted with the more robust cross-validation approach.
Hence, extreme outliers were contained in the validation fold only once out of multiple evaluations.
By ultimately averaging the error metrics across all folds, their impact is diminished.

As we have seen, the Neural Network (just like all remaining models) lacks the ability to capture the entire price range accurately, yet in theory it is supposed to be flexible enough to approximate any arbitrary function reasonably well.
There exist two possible explanations with very different implications:
%%%
\begin{enumerate}
  \item The model is inherently flawed.
  \item The model is faced with a nearly impossible task.
\end{enumerate}
%%%
In order to discriminate the observations with the highest prices from all other listings, the corresponding feature combinations must be separable in the full $59$-dimensional feature space.
Since this high-dimensional space cannot be visualized, we approximate it with an two-dimensional \emph{embedding} or \emph{latent space}.
If the price outliers are clearly separated from the rest in this embedded space, the network is faced with a feasible task.
If, however, the outliers are located in the \emph{middle} of the latent distribution, there is little hope to discriminate the most expensive apartments from any of their potentially much lower priced embedded neighbours.

Modern Machine Learning methods provide a large toolbox for low-dimensional embeddings.
Since the project is mainly focused on Deep Learning, we decided to use a \emph{Variatonal Autoencoder} \citep{kingma2014}.
In contrast to deterministic Autoencoders, the VAE contains an additional loss term apart from the usual reconstruction loss that pulls the encoded latent space distribution towards an isotropic multivariate Gaussian distribution.
For this reason, latent space visualizations of the VAE appear to be more spread out and output classes (that are not used for training the VAE) tend to be easier to identify.

%%%
\begin{figure}[t]
  \centering
  \includegraphics[width=0.8\textwidth]{latent_representation.png}
  \caption{Feature Representation in a two-dimensional latent space embedding}
  \label{fig:latent-representation}
\end{figure}
%%%

\Cref{fig:latent-representation} visualizes the two-dimensional feature embedding.
Already a \emph{single} dimension seems so be sufficient for a general understanding of price segments.
While some of the most expensive apartments are located on the far left, surrounded by highly-priced neighbours, the other half shares a similar feature representation to \emph{medium to low} priced listings.
Hence, the network likely maps this second half to comparably low price predictions resulting in large residuals with a high influence on the MAE and the $R^2$ value.

Assuming that the two-dimensional picture appropriately reflects the situation in the full feature space, there are two possible explanations for this lack of separability:
%%%
\begin{enumerate}
  \item The collected data is simply not rich or expressive enough to capture all factors that contribute to very high prices.
  \item Some apartments are listed at a price that does not represent their true value.
\end{enumerate}
%%%

In the latter case, it can be argued that the entire task of predicting Airbnb prices is questionable in the first place:
In order to draw connections from features to outputs, the process implicitly assumes observations whose listed price can be justified and explained by characteristics of the joint feature distribution.

Moreover, discriminating between these two options is nontrivial.
One idea is to use features like \emph{availability} or the \emph{number of reviews} to approximate demand and classify price outliers with low demand as overpriced.
However, it is entirely possible that an extremely expensive observation with very low demand is listed according to its true value, yet nobody is willing to pay such a high amount for an accomodation, regardless of its quality.
Since such accommodations might very well be contained in a new data during deployment, the model should have learned to handle them appropriately during training.
As a consequence, a blind removal of such outliers is difficult to justify and narrows down the set of feasible target distributions.


\subsection{Price Predictions on Munich Dataset}

In addition, we tested our implemented models on the Munich dataset, which was also published by the \emph{Inside Airbnb} project.
This dataset includes twice as many accommodations as the previous dataset for Oslo. As expected, the flexible models, such as Gradient Boosting and the Neural Network, have benefited most from this increase in data size. Nevertheless, the overall performance is not significantly better compared to the originally used Oslo data.
The MAE of the Neural Network on the validation data has decreased from $44.3$ Euros for Oslo to $42.5$ Euros for Munich. To further improve the performance of this model, a deeper and broader architecture of the network could be considered, specifically designed for the larger data set.
Overall, the Gradient Boosting model performed the best on the test set with a MAE of $32.8$ Euros and a $R^2$ of $0.453$. For comparison, using the Oslo data, the MAE of the model was $41.2$ Euros.
The most important predictors of the Munich data set were \emph{Property Type}, \emph{Neighborhood}, and \emph{Accommodates}. Furthermore, this dataset also contains some outliers, whose influence is shown in \Cref{tab:munich-outliers}. Excluding the top $1\%$ of prices when fitting the model again leads to a significant improvement in MAE and $R^2$, but not quite as much as was previously the case with the Oslo data.

\begin{table}[t]
  \centering
  \begin{tabular}{@{}ccc@{}}
    \toprule
    Quantile Threshold & MAE   & R2   \\ \midrule
    0.0                & 42.49 & 0.31 \\
    1.0                & 35.32 & 0.44 \\
    2.5                & 30.26 & 0.42 \\
    5.0                & 25.2  & 0.45 \\
    10.0               & 22.94 & 0.4  \\ \bottomrule
  \end{tabular}
  \caption{Mean Absolute Error and $R^2$ value of the Neural Network on the validation set after removing the highest quantiles of the price distribution from the Munich data set}
  \label{tab:munich-outliers}
\end{table}








