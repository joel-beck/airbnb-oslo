{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3WdkVVrfdmx3",
        "outputId": "cef04e7b-ae77-41b3-f6e5-48ea24dc7b63"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/gdrive\")\n",
        "\n",
        "RUN_TRAINING = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Xaoxj-i_yyE",
        "outputId": "2e01688b-6813-4405-8b16-39f4beb65bcb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pandas==1.3 in /usr/local/lib/python3.7/dist-packages (1.3.0)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.7/dist-packages (from pandas==1.3) (1.19.5)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas==1.3) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas==1.3) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas==1.3) (1.15.0)\n",
            "Requirement already satisfied: pickle5 in /usr/local/lib/python3.7/dist-packages (0.0.12)\n"
          ]
        }
      ],
      "source": [
        "# by default colab has pandas 1.1.5 installed, pickle reading below requires same version as when writing, i.e. 1.3\n",
        "!pip install pandas==1.3\n",
        "\n",
        "# colab uses python 3.7, use pickle5 to make pickle processes compatible across versions\n",
        "!pip install pickle5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "filW3spgdYVm"
      },
      "outputs": [],
      "source": [
        "# relative imports are painful in colab, copy all needed functions into this file\n",
        "# colab uses python 3.7 => need List[str] instead of list[str]\n",
        "\n",
        "import time\n",
        "from dataclasses import dataclass, field\n",
        "from pathlib import Path\n",
        "from typing import Any, List, Optional, Tuple, Union\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as mticker\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pickle5 as pickle\n",
        "import seaborn as sns\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "from torch import optim\n",
        "from torch.optim import SGD, Adam\n",
        "from torch.utils.data import DataLoader, Dataset, random_split\n",
        "from torchvision import transforms\n",
        "from torchvision.models import resnet18\n",
        "from io import BytesIO\n",
        "from PIL import Image\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "FsqyMpMsdYVu"
      },
      "outputs": [],
      "source": [
        "# change paths for running on Colab\n",
        "\n",
        "### replace by individual path to project folder ###\n",
        "base_path = Path(\"/content/gdrive/Othercomputers/Mein Laptop/airbnb-oslo\")\n",
        "\n",
        "data_path = base_path / \"data-clean\"\n",
        "listings_path = data_path / \"listings.pkl\"\n",
        "responses_path = data_path / \"front_page_responses.pkl\"\n",
        "\n",
        "with open(listings_path, \"rb\") as f:\n",
        "    listings_df = pickle.load(f)\n",
        "\n",
        "with open(responses_path, \"rb\") as f:\n",
        "    front_page_responses = pickle.load(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "Igmdr4SCA85_"
      },
      "outputs": [],
      "source": [
        "@dataclass\n",
        "class ResultContainer:\n",
        "    \"\"\"\n",
        "    Collects all Results of Interest after the Model Fitting Step in a single Object and displays them in a Pandas DataFrame\n",
        "    \"\"\"\n",
        "\n",
        "    model_names: List[str] = field(default_factory=list)\n",
        "    train_mae_list: List[float] = field(default_factory=list)\n",
        "    val_mae_list: List[float] = field(default_factory=list)\n",
        "    train_r2_list: List[float] = field(default_factory=list)\n",
        "    val_r2_list: List[float] = field(default_factory=list)\n",
        "    train_mse_list: List[float] = field(default_factory=list)\n",
        "    val_mse_list: List[float] = field(default_factory=list)\n",
        "    hyperparam_keys: List[str] = field(default_factory=list)\n",
        "    hyperparam_values: List[float] = field(default_factory=list)\n",
        "    num_features: List[float] = field(default_factory=list)\n",
        "    feature_selector: List[float] = field(default_factory=list)\n",
        "    log_y: List[bool] = field(default_factory=list)\n",
        "\n",
        "    def append(\n",
        "        self,\n",
        "        train_mae: float,\n",
        "        val_mae: float,\n",
        "        train_r2: float,\n",
        "        val_r2: float,\n",
        "        train_mse: float,\n",
        "        val_mse: float,\n",
        "        hyperparam_key: Optional[Union[str, List[str]]] = None,\n",
        "        hyperparam_value: Optional[Union[str, float, List[float]]] = None,\n",
        "        num_features: Optional[int] = None,\n",
        "    ):\n",
        "        self.train_mae_list.append(train_mae)\n",
        "        self.val_mae_list.append(val_mae)\n",
        "        self.train_r2_list.append(train_r2)\n",
        "        self.val_r2_list.append(val_r2)\n",
        "        self.train_mse_list.append(train_mse)\n",
        "        self.val_mse_list.append(val_mse)\n",
        "\n",
        "        if hyperparam_key is not None:\n",
        "            self.hyperparam_keys.append(hyperparam_key)\n",
        "        if hyperparam_value is not None:\n",
        "            self.hyperparam_values.append(hyperparam_value)\n",
        "        if num_features is not None:\n",
        "            self.num_features.append(num_features)\n",
        "\n",
        "    def display_df(self) -> pd.DataFrame:\n",
        "        metrics_df = pd.DataFrame(\n",
        "            {\n",
        "                \"mae_train\": self.train_mae_list,\n",
        "                \"mae_val\": self.val_mae_list,\n",
        "                \"r2_train\": self.train_r2_list,\n",
        "                \"r2_val\": self.val_r2_list,\n",
        "                \"mse_train\": self.train_mse_list,\n",
        "                \"mse_val\": self.val_mse_list,\n",
        "                \"hyperparam_keys\": self.hyperparam_keys,\n",
        "                \"hyperparam_values\": self.hyperparam_values,\n",
        "                \"num_features\": self.num_features,\n",
        "                \"feature_selector\": self.feature_selector,\n",
        "                \"log_y\": self.log_y,\n",
        "            },\n",
        "            index=self.model_names,\n",
        "        )\n",
        "\n",
        "        return metrics_df.sort_values(\"mae_val\")\n",
        "\n",
        "def generate_train_val_data_split(\n",
        "    full_dataset: Dataset, split_seed: int = 123, val_frac: float = 0.2\n",
        ") -> Tuple[Dataset, Dataset]:\n",
        "    \"\"\"\n",
        "    Splits the entire Dataset used for Model Training into a Training Set and Validation Set.\n",
        "    The relative Sizes of each Output Component can be specified with a fractional Value between 0 and 1.\n",
        "    \"\"\"\n",
        "\n",
        "    num_val_samples = np.ceil(val_frac * len(full_dataset)).astype(int)\n",
        "    num_train_samples = len(full_dataset) - num_val_samples\n",
        "    trainset, valset = random_split(\n",
        "        dataset=full_dataset,\n",
        "        lengths=(num_train_samples, num_val_samples),\n",
        "        generator=torch.Generator().manual_seed(split_seed),\n",
        "    )\n",
        "    return trainset, valset\n",
        "\n",
        "@dataclass\n",
        "class NeuralNetMetrics:\n",
        "    \"\"\"\n",
        "    Tracks Performance Metrics for all Epochs during Model Training.\n",
        "    This is particularly useful to plot Loss Curves of the Mean Squared Error, Mean Absolute Error and R^2 Value on Training and Validation Set after Training is completed.\n",
        "    \"\"\"\n",
        "\n",
        "    train_losses: List[str] = field(default_factory=list)\n",
        "    val_losses: List[float] = field(default_factory=list)\n",
        "    train_maes: List[float] = field(default_factory=list)\n",
        "    val_maes: List[float] = field(default_factory=list)\n",
        "    train_r2s: List[float] = field(default_factory=list)\n",
        "    val_r2s: List[float] = field(default_factory=list)\n",
        "\n",
        "    def append(\n",
        "        self,\n",
        "        train_loss: float,\n",
        "        val_loss: float,\n",
        "        train_mae: float,\n",
        "        val_mae: float,\n",
        "        train_r2: float,\n",
        "        val_r2: float,\n",
        "    ):\n",
        "        self.train_losses.append(train_loss)\n",
        "        self.val_losses.append(val_loss)\n",
        "        self.train_maes.append(train_mae)\n",
        "        self.val_maes.append(val_mae)\n",
        "        self.train_r2s.append(train_r2)\n",
        "        self.val_r2s.append(val_r2)\n",
        "\n",
        "    def plot(self):\n",
        "        sns.set_theme(style=\"whitegrid\")\n",
        "\n",
        "        fig, (ax1, ax2, ax3) = plt.subplots(nrows=3, figsize=(9, 9))\n",
        "\n",
        "        epochs = range(1, len(self.train_losses) + 1)\n",
        "\n",
        "        ax1.plot(epochs, self.train_losses, label=\"Training\") #, marker=\"o\")\n",
        "        ax1.plot(epochs, self.val_losses, label=\"Validation\") #, marker=\"o\")\n",
        "        ax1.set(\n",
        "            title=\"Mean Squared Error\",\n",
        "            xlabel=\"\",\n",
        "            ylabel=\"\",\n",
        "        )\n",
        "        ax1.legend()\n",
        "\n",
        "        ticks_loc = ax1.get_yticks().tolist()\n",
        "        ax1.yaxis.set_major_locator(mticker.FixedLocator(ticks_loc))\n",
        "        ax1.set_yticklabels([\"{:,}\".format(int(x)) for x in ticks_loc])\n",
        "\n",
        "        ax2.plot(epochs, self.train_maes, label=\"Training\")#, marker=\"o\")\n",
        "        ax2.plot(epochs, self.val_maes, label=\"Validation\")#, marker=\"o\")\n",
        "        ax2.set(\n",
        "            title=\"Mean Absolute Error\",\n",
        "            xlabel=\"\",\n",
        "            ylabel=\"\",\n",
        "        )\n",
        "\n",
        "        ax3.plot(epochs, self.train_r2s, label=\"Training\")#, marker=\"o\")\n",
        "        ax3.plot(epochs, self.val_r2s, label=\"Validation\")#, marker=\"o\")\n",
        "        ax3.set(\n",
        "            title=\"R2\",\n",
        "            xlabel=\"Epoch\",\n",
        "            ylabel=\"\",\n",
        "        )\n",
        "\n",
        "        fig.tight_layout()\n",
        "        sns.move_legend(\n",
        "            obj=ax1, loc=\"upper center\", bbox_to_anchor=(1.1, -0.7), frameon=False\n",
        "        )\n",
        "\n",
        "        sns.despine()\n",
        "        plt.show()\n",
        "\n",
        "\n",
        "def init_weights(layer: nn.Module, mean: float = 0, std: float = 1):\n",
        "    \"\"\"\n",
        "    Initializes Model Weights at the Start of Training.\n",
        "    Avoids negative predicted Prices during the first Epochs to take the Logarithm when training with Log-Prices.\n",
        "    \"\"\"\n",
        "\n",
        "    if isinstance(layer, nn.Linear):\n",
        "        # avoid negative predicted prices at beginning of training to enable log transformation\n",
        "        torch.nn.init.normal_(layer.weight, mean=mean, std=std)\n",
        "\n",
        "\n",
        "def train_regression(\n",
        "    dataloader: DataLoader,\n",
        "    optimizer: Union[Adam, SGD],\n",
        "    model: Any,\n",
        "    loss_function: nn.MSELoss,\n",
        "    device: torch.device,\n",
        "    log_y: bool = False,\n",
        ") -> Tuple[float, float, float]:\n",
        "    \"\"\"\n",
        "    Model Training and Weight Adjustment Step on the Training Set for a single Epoch.\n",
        "    Returns Mean Squared Error, Mean Absolute Error and R^2 Value for this Epoch on the Training Set.\n",
        "    \"\"\"\n",
        "\n",
        "    # calculate mean squared error, mean_absolute error and r2 with values of all batches to perform comparable computations as with classical models\n",
        "    y_true_list = []\n",
        "    y_pred_list = []\n",
        "\n",
        "    model.train()\n",
        "    for x, y in dataloader:\n",
        "        x = x.to(device=device)\n",
        "        y = y.to(device=device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        y_pred = model(x).squeeze()\n",
        "\n",
        "        # train model / backpropagate loss on log scale\n",
        "        if log_y:\n",
        "            y = torch.log(y)\n",
        "            y_pred = torch.log(y_pred)\n",
        "\n",
        "        loss = loss_function(y_pred, y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # collect error metrics on original scale\n",
        "        if log_y:\n",
        "            y = torch.exp(y)\n",
        "            y_pred = torch.exp(y_pred)\n",
        "\n",
        "        y_true_list.extend(list(y.detach().cpu()))\n",
        "        y_pred_list.extend(list(y_pred.detach().cpu()))\n",
        "\n",
        "    mse = mean_squared_error(y_true_list, y_pred_list)\n",
        "    mae = mean_absolute_error(y_true_list, y_pred_list)\n",
        "    r2 = r2_score(y_true_list, y_pred_list)\n",
        "\n",
        "    return mse, mae, r2\n",
        "\n",
        "\n",
        "def validate_regression(\n",
        "    dataloader: DataLoader, model: Any, device: torch.device\n",
        ") -> Tuple[float, float, float]:\n",
        "    \"\"\"\n",
        "    Evaluation Step on the Validation Set for a single Epoch.\n",
        "    Returns Mean Squared Error, Mean Absolute Error and R^2 Value for this Epoch on the Validation Set.\n",
        "    \"\"\"\n",
        "\n",
        "    y_true_list = []\n",
        "    y_pred_list = []\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for x, y in dataloader:\n",
        "            x = x.to(device=device, dtype=torch.float)\n",
        "            y = y.to(device=device, dtype=torch.float)\n",
        "\n",
        "            y_pred = model(x).squeeze()\n",
        "\n",
        "            y_true_list.extend(list(y.detach().cpu()))\n",
        "            y_pred_list.extend(list(y_pred.detach().cpu()))\n",
        "\n",
        "    mse = mean_squared_error(y_true_list, y_pred_list)\n",
        "    mae = mean_absolute_error(y_true_list, y_pred_list)\n",
        "    r2 = r2_score(y_true_list, y_pred_list)\n",
        "\n",
        "    return mse, mae, r2\n",
        "\n",
        "\n",
        "def print_epoch(\n",
        "    epoch: int,\n",
        "    num_epochs: int,\n",
        "    epoch_train_mse: float,\n",
        "    epoch_val_mse: float,\n",
        "    epoch_train_mae: float,\n",
        "    epoch_val_mae: float,\n",
        "    epoch_train_r2: float,\n",
        "    epoch_val_r2: float,\n",
        "    scheduler: Optional[Any] = None,\n",
        "):\n",
        "    \"\"\"\n",
        "    Prints Information about the Model Performance in Training and Validation Set of the current Epoch while the Model is trained.\n",
        "    \"\"\"\n",
        "\n",
        "    print(f\"Epoch: {epoch} / {num_epochs}\\n{'-' * 50}\")\n",
        "    if scheduler is not None:\n",
        "        print(f\"Learning Rate: {scheduler.state_dict()['_last_lr'][0]:.1e}\")\n",
        "    print(\n",
        "        f\"Mean MSE Training: {epoch_train_mse:.3f} | Mean MSE Validation: {epoch_val_mse:.3f}\\n\"\n",
        "        f\"Mean MAE Training: {epoch_train_mae:.3f} | Mean MAE Validation: {epoch_val_mae:.3f}\\n\"\n",
        "        f\"Mean R2 Training: {epoch_train_r2:.3f} | Mean R2 Validation: {epoch_val_r2:.3f}\\n\"\n",
        "    )\n",
        "\n",
        "\n",
        "def print_best(\n",
        "    best_train_mae: float,\n",
        "    best_train_mae_epoch: int,\n",
        "    best_val_mae: float,\n",
        "    best_val_mae_epoch: int,\n",
        "):\n",
        "    \"\"\"\n",
        "    Prints the lowest Mean Absolute Error of the Training and Validation Set encountered during Model Training after Training is completed.\n",
        "    \"\"\"\n",
        "\n",
        "    print(\n",
        "        f\"\\nBest Mean MAE Training: {best_train_mae:.3f} (Epoch {best_train_mae_epoch})\"\n",
        "        f\"\\nBest Mean MAE Validation: {best_val_mae:.3f} (Epoch {best_val_mae_epoch})\"\n",
        "    )\n",
        "\n",
        "\n",
        "def run_regression(\n",
        "    model: Any,\n",
        "    optimizer: Union[Adam, SGD],\n",
        "    loss_function: nn.MSELoss,\n",
        "    device: torch.device,\n",
        "    num_epochs: int,\n",
        "    train_dataloader: DataLoader,\n",
        "    val_dataloader: DataLoader,\n",
        "    result_container: Optional[ResultContainer] = None,\n",
        "    log_y: bool = False,\n",
        "    scheduler: Optional[Any] = None,\n",
        "    save_best: bool = False,\n",
        "    save_path: bool = None,\n",
        "    verbose: bool = False,\n",
        ") -> Union[Tuple[NeuralNetMetrics, ResultContainer], NeuralNetMetrics]:\n",
        "    \"\"\"\n",
        "    Trains a Neural Network Regression Model for a specified number of Epochs.\n",
        "    During Training Performance Information is displayed.\n",
        "    Optionally the State with the lowest Mean Absolute Error can be tracked and saved.\n",
        "\n",
        "    Returns a NeuralNetMetrics Object for plotting Loss Curves and optionally a ResultContainer Object to collect the Results in a Pandas DataFrame for Comparison with the Classical Statistical Models.\n",
        "    \"\"\"\n",
        "\n",
        "    start_time = time.perf_counter()\n",
        "    metrics = NeuralNetMetrics()\n",
        "\n",
        "    if save_best:\n",
        "        # use mean absolute error as metric for early stopping\n",
        "        best_train_mae = np.inf\n",
        "        best_val_mae = np.inf\n",
        "\n",
        "    for epoch in range(1, num_epochs + 1):\n",
        "\n",
        "        epoch_train_mse, epoch_train_mae, epoch_train_r2 = train_regression(\n",
        "            dataloader=train_dataloader,\n",
        "            optimizer=optimizer,\n",
        "            model=model,\n",
        "            loss_function=loss_function,\n",
        "            device=device,\n",
        "            log_y=log_y,\n",
        "        )\n",
        "\n",
        "        if scheduler is not None:\n",
        "            scheduler.step()\n",
        "\n",
        "        epoch_val_mse, epoch_val_mae, epoch_val_r2 = validate_regression(\n",
        "            dataloader=val_dataloader,\n",
        "            model=model,\n",
        "            device=device,\n",
        "        )\n",
        "\n",
        "        metrics.append(\n",
        "            epoch_train_mse,\n",
        "            epoch_val_mse,\n",
        "            epoch_train_mae,\n",
        "            epoch_val_mae,\n",
        "            epoch_train_r2,\n",
        "            epoch_val_r2,\n",
        "        )\n",
        "\n",
        "        if save_best:\n",
        "            if epoch_train_mae < best_train_mae:\n",
        "                best_train_mae_epoch = epoch\n",
        "                best_train_mae = epoch_train_mae\n",
        "\n",
        "            if epoch_val_mae < best_val_mae:\n",
        "                best_val_mae_epoch = epoch\n",
        "                best_val_mae = epoch_val_mae\n",
        "\n",
        "                # save weights for lowest validation mae\n",
        "                if save_path is not None:\n",
        "                    torch.save(model.state_dict(), save_path)\n",
        "\n",
        "        if verbose:\n",
        "            if epoch % int(num_epochs / 5) == 0:\n",
        "                print_epoch(\n",
        "                    epoch,\n",
        "                    num_epochs,\n",
        "                    epoch_train_mse,\n",
        "                    epoch_val_mse,\n",
        "                    epoch_train_mae,\n",
        "                    epoch_val_mae,\n",
        "                    epoch_train_r2,\n",
        "                    epoch_val_r2,\n",
        "                    scheduler,\n",
        "                )\n",
        "\n",
        "    time_elapsed = np.round(time.perf_counter() - start_time, 0).astype(int)\n",
        "    print(f\"Finished training after {time_elapsed} seconds.\")\n",
        "\n",
        "    # check twice for save_best to include both cases for result_container is None and result_container is not None\n",
        "    if save_best:\n",
        "        print_best(\n",
        "            best_train_mae, best_train_mae_epoch, best_val_mae, best_val_mae_epoch\n",
        "        )\n",
        "\n",
        "    if result_container is None:\n",
        "        return metrics\n",
        "    \n",
        "    result_container.log_y.append(log_y)\n",
        "\n",
        "    if save_best:\n",
        "        # if save_best=True save results from epoch with best validation mae (starts at epoch=1)\n",
        "        result_container.append(\n",
        "            metrics.train_maes[best_val_mae_epoch - 1],\n",
        "            metrics.val_maes[best_val_mae_epoch - 1],\n",
        "            metrics.train_r2s[best_val_mae_epoch - 1],\n",
        "            metrics.val_r2s[best_val_mae_epoch - 1],\n",
        "            metrics.train_losses[best_val_mae_epoch - 1],\n",
        "            metrics.val_losses[best_val_mae_epoch - 1],\n",
        "        )\n",
        "    else:\n",
        "        # if save_best=False save result from last epoch (starts at epoch=1)\n",
        "        result_container.append(\n",
        "            metrics.train_maes[epoch - 1],\n",
        "            metrics.val_maes[epoch - 1],\n",
        "            metrics.train_r2s[epoch - 1],\n",
        "            metrics.val_r2s[epoch - 1],\n",
        "            metrics.train_losses[epoch - 1],\n",
        "            metrics.val_losses[epoch - 1],\n",
        "        )\n",
        "\n",
        "    return metrics, result_container\n",
        "\n",
        "  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "4YydujDLdYVx"
      },
      "outputs": [],
      "source": [
        "# drop prices of 0, largest price (outlier) and missing images\n",
        "response_price_df = (\n",
        "    pd.merge(\n",
        "        front_page_responses, listings_df[\"price\"], left_index=True, right_index=True\n",
        "    )\n",
        "    .rename(columns={\"listing_url\": \"front_page_responses\"})\n",
        "    .loc[lambda x: (x[\"price\"] > 0) & (x[\"price\"] < 80000)]\n",
        "    .dropna()\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "Lfm9fDpdmmdH"
      },
      "outputs": [],
      "source": [
        "# remove all images that lead to error when opening to not interrupt pytorch training loop later\n",
        "def open_image(response):\n",
        "  try:\n",
        "    img = Image.open(BytesIO(response.content))\n",
        "  except:\n",
        "    img = pd.NA\n",
        "  return img\n",
        "\n",
        "response_price_df = response_price_df.assign(\n",
        "    front_page_pictures = response_price_df[\"front_page_responses\"].apply(open_image)\n",
        "    ).dropna()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "T0GfEsRSdYV2"
      },
      "outputs": [],
      "source": [
        "IMAGE_SIZE = [224, 224]\n",
        "batch_size = 128\n",
        "\n",
        "image_transforms = transforms.Compose(\n",
        "    [transforms.Resize(size=IMAGE_SIZE), transforms.PILToTensor()]\n",
        ")\n",
        "\n",
        "# normalization requires four dimensions, done after unsqueezing\n",
        "# values of channel means and standard deviations from documentation for resnet https://pytorch.org/vision/stable/models.html\n",
        "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jIK-a5_xdYV7",
        "outputId": "4e09fe23-4f6c-4648-c5ec-ca2f7347cf72"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "device = torch.device(device=\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "3ec3n2Q0dYV9"
      },
      "outputs": [],
      "source": [
        "class ListingsImages(Dataset):\n",
        "    \"\"\"\n",
        "    Creates PyTorch Dataset from Pandas DataFrame containing (at least) one Column of Picture URLs and one additional Column of corresponding Prices.\n",
        "    The resulting Images are resized and normalized four-dimensional Tensors and serve, together with the returned Price Tensors, as Input to a PyTorch DataLoader object.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, df, image_transforms=None):\n",
        "        # still use requests responses instead of images themselves to avoid RAM overflow\n",
        "        self.x = df[\"front_page_responses\"]\n",
        "        self.y = torch.tensor(df[\"price\"].values, dtype=torch.float)\n",
        "        self.image_transforms = image_transforms\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        response = self.x.iloc[index]\n",
        "        img = Image.open(BytesIO(response.content))\n",
        "        label = self.y[index]\n",
        "\n",
        "        if self.image_transforms is not None:\n",
        "            img_tensor = self.image_transforms(img).to(dtype=torch.float)\n",
        "            # some images have four color channels for some reason\n",
        "            img_tensor = img_tensor[:3, :, :]\n",
        "            img_tensor = normalize(img_tensor)\n",
        "\n",
        "        return img_tensor, label\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.y)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "QHJntNe8dYWA"
      },
      "outputs": [],
      "source": [
        "full_dataset = ListingsImages(response_price_df, image_transforms)\n",
        "trainset, valset = generate_train_val_data_split(full_dataset)\n",
        "\n",
        "trainloader = DataLoader(trainset, batch_size=batch_size, shuffle=True)\n",
        "valloader = DataLoader(valset, batch_size=batch_size, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "vsVBX2sUdYWD"
      },
      "outputs": [],
      "source": [
        "model = resnet18(pretrained=True)\n",
        "\n",
        "# freeze weights\n",
        "for param in model.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "# replace last fully connected layer, weights of new layer require gradient computation\n",
        "in_features = model.fc.in_features\n",
        "model.fc = nn.Linear(in_features, 1)\n",
        "\n",
        "model.to(device=device)\n",
        "\n",
        "params_to_update = [param for param in model.parameters() if param.requires_grad]\n",
        "\n",
        "lr = 0.01\n",
        "# takes 3 minutes per Epoch on Colab :/\n",
        "num_epochs = 10\n",
        "save_path = base_path / \"models/cnn_weights.pt\"\n",
        "\n",
        "# use only parameters with requires_grad = True in optimizer\n",
        "optimizer = optim.Adam(params_to_update, lr=lr)\n",
        "loss_function = nn.MSELoss()\n",
        "\n",
        "result_container = ResultContainer()\n",
        "\n",
        "if RUN_TRAINING:\n",
        "    metrics = run_regression(\n",
        "        model,\n",
        "        optimizer,\n",
        "        loss_function,\n",
        "        device,\n",
        "        num_epochs,\n",
        "        trainloader,\n",
        "        valloader,\n",
        "        verbose=True,\n",
        "        save_best=True,\n",
        "        save_path=save_path\n",
        "    ) \n",
        "    metrics.plot()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VJ1_lu52wz3x",
        "outputId": "f5f49e9a-1038-4903-9fe7-150d4c1b93cc"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model = resnet18(pretrained=True)\n",
        "\n",
        "# freeze weights\n",
        "for param in model.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "# replace last fully connected layer, weights of new layer require gradient computation\n",
        "in_features = model.fc.in_features\n",
        "model.fc = nn.Linear(in_features, 1)\n",
        "\n",
        "model.to(device=device)\n",
        "model.load_state_dict(torch.load(save_path))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "coFmjPXpUAqo"
      },
      "outputs": [],
      "source": [
        "# make predictions on entire dataset, set shuffle to False to keep same order as in original dataframe\n",
        "dataloader = DataLoader(full_dataset, batch_size=128, shuffle=False)\n",
        "predicted_prices = []\n",
        "\n",
        "# not tested again, maybe use .item() somewhere to extract value out of tensors\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    for images, prices in dataloader:\n",
        "        images = images.to(device=device)\n",
        "        batch_predictions = model(images)\n",
        "        predicted_prices.extend(batch_predictions)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "Ylg1BZLEzHae"
      },
      "outputs": [],
      "source": [
        "# new_predictions = []\n",
        "# \n",
        "# for sublist in predicted_prices:\n",
        "#   for num in sublist:\n",
        "#     new_predictions.append(num.item())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VCwbqIBSXdj_",
        "outputId": "6f5f9f48-ef14-478a-fe85-2967bc644188"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "id\n",
              "42932        977.533142\n",
              "42932        964.276978\n",
              "42932        855.076843\n",
              "42932       1038.461182\n",
              "42932        932.958862\n",
              "               ...     \n",
              "52523033     933.351868\n",
              "52523033    1037.093994\n",
              "52523033     814.780151\n",
              "52523033     890.354736\n",
              "52523033     971.215942\n",
              "Length: 14374, dtype: float64"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "predictions_series = pd.Series(\n",
        "    predicted_prices, index = response_price_df.index\n",
        "  )\n",
        "predictions_series"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "JhqYT56hX5cw"
      },
      "outputs": [],
      "source": [
        "predictions_series.to_pickle(base_path / \"data-clean/cnn_predictions.pkl\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "cnn_pretrained_colab.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
