Index: models/pytorch_helpers.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>import time\n\nimport matplotlib.pyplot as plt\nfrom matplotlib.ticker import ScalarFormatter\nimport numpy as np\nimport seaborn as sns\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Subset, random_split\n\n\ndef generate_train_val_data_split(full_dataset, split_seed=123, val_frac=0.2):\n    num_val_samples = np.ceil(val_frac * len(full_dataset)).astype(int)\n    num_train_samples = len(full_dataset) - num_val_samples\n    trainset, valset = random_split(\n        dataset=full_dataset,\n        lengths=(num_train_samples, num_val_samples),\n        generator=torch.Generator().manual_seed(split_seed),\n    )\n    return trainset, valset\n\n\ndef generate_subsets(trainset, valset, subset_size):\n    train_indices = torch.randint(0, len(trainset) + 1, size=(subset_size,))\n    trainset = Subset(dataset=trainset, indices=train_indices)\n\n    val_indices = torch.randint(0, len(valset) + 1, size=(subset_size,))\n    valset = Subset(dataset=valset, indices=val_indices)\n\n    return trainset, valset\n\n\ndef init_data_loaders(trainset, valset, testset, batch_size=64):\n    trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True)\n    valloader = torch.utils.data.DataLoader(valset, batch_size=batch_size, shuffle=True)\n    testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size, shuffle=True)\n\n    return trainloader, valloader, testloader\n\n\ndef print_param_shapes(model, col_widths=(25, 8)):\n    for name, param in model.named_parameters():\n        print(\n            f\"Name: {name:<{col_widths[0]}} | # Params: {param.numel():<{col_widths[1]}} | Shape: {list(param.shape)}\"\n        )\n    print(\"\\nTotal number of parameters:\", sum(p.numel() for p in model.parameters()))\n\n\ndef _print_shape(input, layer=None, col_width=25):\n    if layer is None:\n        print(f\"{f'Input shape:':<{col_width}} {list(input.shape)}\")\n    else:\n        print(f\"{f'{layer.__class__.__name__} output shape:':<25} {list(input.shape)}\")\n\n\ndef print_data_shapes(model, device, input_shape, exclude=nn.Sequential):\n    x = torch.rand(size=input_shape, dtype=torch.float32).to(device=device)\n    _print_shape(x)\n\n    for i, layer in enumerate(model.modules()):\n        if i == 0:\n            continue\n\n        elif isinstance(layer, exclude):\n            print(\"-\" * 20)\n            print(f\"{layer.__class__.__name__} layer:\")\n\n        else:\n            x = layer(x)\n            _print_shape(x, layer)\n\n\ndef train_regression(dataloader, optimizer, model, loss_function, device):\n    # epoch_loss, epoch_total = 0.0, 0.0\n    epoch_loss = []\n\n    for x, y in dataloader:\n\n        x = x.to(device=device)\n        y = y.to(device=device)\n\n        optimizer.zero_grad()\n        model.train()\n\n        y_pred = model(x).squeeze()\n        #batch_size = len(y)\n        #epoch_total += batch_size\n\n        # total loss per sample in minibatch (sum of squared deviations)\n        # when using MSELoss(reduction=\"sum\")\n        loss = loss_function(y_pred, y)\n        # epoch_loss += loss\n\n        loss.backward()\n        optimizer.step()\n\n        epoch_loss.append(loss.item())\n\n    # return mean loss per sample in whole dataset\n    # return epoch_loss.detach().to(device=\"cpu\").numpy() / epoch_total\n    return np.mean(epoch_loss)\n\n\ndef validate_regression(dataloader, model, loss_function, device):\n    #epoch_loss, epoch_total = 0.0, 0.0\n    epoch_loss = []\n\n    model.eval()\n    with torch.no_grad():\n        for x, y in dataloader:\n\n            x = x.to(device=device)\n            y = y.to(device=device)\n\n            y_pred = model(x).squeeze()\n            #batch_size = len(y)\n            #epoch_total += batch_size\n\n            loss = loss_function(y_pred, y)\n            #epoch_loss += loss\n            epoch_loss.append(loss.item())\n\n    #return epoch_loss.detach().to(device=\"cpu\").numpy() / epoch_total\n    return np.mean(epoch_loss)\n\ndef run_regression(\n    model,\n    optimizer,\n    loss_function,\n    device,\n    num_epochs,\n    train_dataloader,\n    val_dataloader,\n    save_best=False,\n    save_path=None,\n    verbose=False,\n):\n    start_time = time.perf_counter()\n    train_losses, val_losses = [], []\n\n    if save_best:\n        best_loss_train = np.inf\n        best_loss_val = np.inf\n\n    for epoch in range(1, num_epochs + 1):\n\n        epoch_train_loss = train_regression(\n            dataloader=train_dataloader,\n            optimizer=optimizer,\n            model=model,\n            loss_function=loss_function,\n            device=device,\n        )\n        epoch_val_loss = validate_regression(\n            dataloader=val_dataloader,\n            model=model,\n            loss_function=loss_function,\n            device=device,\n        )\n\n        train_losses.append(epoch_train_loss)\n        val_losses.append(epoch_val_loss)\n\n        if save_best:\n            if epoch_train_loss < best_loss_train:\n                best_loss_train_epoch = epoch\n                best_loss_train = epoch_train_loss\n\n            if epoch_val_loss < best_loss_val:\n                best_loss_val_epoch = epoch\n                best_loss_val = epoch_val_loss\n\n                # save weights for lowest validation loss\n                if save_path is not None:\n                    torch.save(model.state_dict(), save_path)\n\n        if verbose:\n            if epoch % int(num_epochs / 5) == 0:\n                print(f\"Epoch: {epoch} / {num_epochs}\\n{'-' * 50}\")\n                print(\n                    f\"Mean Loss Training: {epoch_train_loss:.5f} | Mean Loss Validation: {epoch_val_loss:.5f}\\n\"\n                )\n\n    time_elapsed = np.round(time.perf_counter() - start_time, 0).astype(int)\n    print(f\"Finished training after {time_elapsed} seconds.\")\n\n    if save_best:\n        print(\n            f\"\\nBest Mean Loss Training: {best_loss_train:.3f} (Epoch {best_loss_train_epoch})\"\n        )\n        print(\n            f\"Best Mean Loss Validation: {best_loss_val:.3f} (Epoch {best_loss_val_epoch})\"\n        )\n\n    return train_losses, val_losses\n\n\ndef plot_regression(train_losses, val_losses):\n    sns.set_theme(style=\"whitegrid\")\n\n    fig, ax = plt.subplots(figsize=(10, 5))\n\n    epochs = range(1, len(train_losses) + 1)\n\n    ax.plot(epochs, train_losses, label=\"Training\", marker=\"o\")\n    ax.plot(epochs, val_losses, label=\"Validation\", marker=\"o\")\n    ax.set(\n        title=\"Loss\",\n        xlabel=\"Epoch\",\n        ylabel=\"\",\n    )\n    ax.legend()\n    # ax.ticklabel_format(useOffset=False)\n    # ax.yaxis.set_major_formatter(ScalarFormatter(useOffset=False)) # tried to delete the 1e* on top of the plot\n\n    sns.despine()\n    plt.show()\n\n\ndef accuracy(correct, total):\n    return np.round(correct / total, decimals=3)\n\n\ndef train_classification(dataloader, optimizer, model, loss_fn, device):\n\n    # option 1: save epoch losses in list and return mean of this list\n    # -> creates unnecessary list structure for each epoch\n    # -> samples might not have equal weights if len(dataset) % batch_size != 0, can be sensitive to outliers when samples are shuffled (worst case: last batch contains a single sample, which is an outlier with high loss )\n    # option 2: save total epoch loss and length of dataset 8needed for accuracy as well) and return mean sample loss (or mean batch loss when multiplied with batch_size)\n\n    epoch_loss, epoch_correct, epoch_total = 0.0, 0.0, 0.0\n\n    for x, y in dataloader:\n        x = x.to(device=device)\n        y = y.to(device=device)\n\n        optimizer.zero_grad()\n        model.train()\n\n        y_pred = model(x)\n        class_pred = y_pred.argmax(dim=1)\n\n        batch_correct = (y == class_pred).sum().item()\n        batch_size = len(y)\n\n        epoch_correct += batch_correct\n        epoch_total += batch_size\n\n        loss = loss_fn(y_pred, y)\n        # Cross Entropy Loss calculates mean loss per sample in batch with default: reduction = \"mean\"\n        epoch_loss += loss * batch_size\n\n        loss.backward()\n        optimizer.step()\n\n    return (\n        epoch_loss.detach().to(device=\"cpu\").numpy() / epoch_total,\n        accuracy(epoch_correct, epoch_total),\n    )\n\n\ndef validate_classification(dataloader, model, loss_fn, device):\n\n    epoch_loss, epoch_correct, epoch_total = 0.0, 0.0, 0.0\n\n    model.eval()\n    with torch.no_grad():\n        for x, y in dataloader:\n            x = x.to(device=device)\n            y = y.to(device=device)\n\n            y_pred = model(x)\n            class_pred = y_pred.argmax(dim=1)\n\n            batch_correct = (y == class_pred).sum().item()\n            batch_size = len(y)\n\n            epoch_correct += batch_correct\n            epoch_total += batch_size\n\n            loss = loss_fn(y_pred, y)\n            # Cross Entropy Loss calculates mean loss per sample in batch with default: reduction = \"mean\"\n            epoch_loss += loss * batch_size\n\n    return (\n        epoch_loss.detach().to(device=\"cpu\").numpy() / epoch_total,\n        accuracy(epoch_correct, epoch_total),\n    )\n\n\ndef run_classification(\n    model,\n    optimizer,\n    loss_function,\n    device,\n    num_epochs,\n    train_dataloader,\n    val_dataloader,\n    scheduler=None,\n    save_best=False,\n    save_path=None,\n    early_stopper=None,\n    verbose=False,\n):\n\n    start_time = time.perf_counter()\n    train_losses, val_losses, train_accs, val_accs = [], [], [], []\n    if save_best:\n        best_loss = np.inf\n        best_accuracy = 0.0\n\n    for epoch in range(1, num_epochs + 1):\n\n        epoch_train_loss, epoch_train_acc = train_classification(\n            dataloader=train_dataloader,\n            optimizer=optimizer,\n            model=model,\n            loss_fn=loss_function,\n            device=device,\n        )\n        epoch_val_loss, epoch_val_acc = validate_classification(\n            dataloader=val_dataloader,\n            model=model,\n            loss_fn=loss_function,\n            device=device,\n        )\n\n        train_losses.append(epoch_train_loss)\n        val_losses.append(epoch_val_loss)\n        train_accs.append(epoch_train_acc)\n        val_accs.append(epoch_val_acc)\n\n        if scheduler is not None:\n            schedulers = {\n                \"StepLR\": [],\n                \"ExponentialLR\": [],\n                \"ReduceLROnPlateau\": [epoch_val_loss],\n            }\n            params = schedulers[scheduler.__class__.__name__]\n            scheduler.step(*params)\n\n        if save_best:\n            if epoch_val_loss < best_loss:\n                best_loss_epoch = epoch\n                best_loss = epoch_val_loss\n\n            if epoch_val_acc > best_accuracy:\n                best_acc_epoch = epoch\n                best_accuracy = epoch_val_acc\n\n                if save_path is not None:\n                    torch.save(model.state_dict(), save_path)\n\n        if early_stopper is not None:\n            early_stopper.update(val_acc=epoch_val_acc, model=model)\n            if early_stopper.early_stop:\n                break\n\n        if verbose:\n            if epoch % int(num_epochs / 5) == 0:\n                print(f\"Epoch: {epoch} / {num_epochs}\\n{'-' * 50}\")\n                if scheduler is not None:\n                    print(f\"Learning Rate: {scheduler.state_dict()['_last_lr'][0]:.1e}\")\n                print(\n                    f\"Mean Loss Training: {epoch_train_loss:.5f} | Mean Loss Validation: {epoch_val_loss:.5f}\\n\"\n                    f\"Training Accuracy: {100 * epoch_train_acc:.1f}% | Validation Accuracy: {100 * epoch_val_acc:.1f}%\\n\"\n                )\n\n    time_elapsed = np.round(time.perf_counter() - start_time, 0).astype(int)\n    print(f\"Finished training after {time_elapsed} seconds.\")\n\n    if save_best:\n        print(\n            f\"\\nBest Mean Loss Validation: {best_loss:.3f} (Epoch {best_loss_epoch})\\n\"\n            f\"Best Validation Accuracy: {100 * best_accuracy:.1f}% (Epoch {best_acc_epoch})\"\n        )\n\n    return train_losses, val_losses, train_accs, val_accs\n\n\ndef plot_classification(train_losses, val_losses, train_accs, val_accs):\n    sns.set_theme(style=\"whitegrid\")\n\n    fig, (ax1, ax2) = plt.subplots(nrows=1, ncols=2, figsize=(10, 5))\n\n    epochs = range(1, len(train_losses) + 1)\n\n    ax1.plot(epochs, train_losses, label=\"Training\", marker=\"o\")\n    ax1.plot(epochs, val_losses, label=\"Validation\", marker=\"o\")\n    ax1.set(\n        title=\"Loss\",\n        xlabel=\"Epoch\",\n        ylabel=\"\",\n    )\n    ax1.legend()\n\n    ax2.plot(epochs, train_accs, label=\"Training\", marker=\"o\")\n    ax2.plot(epochs, val_accs, label=\"Validation\", marker=\"o\")\n    ax2.set(\n        title=\"Accuracy\",\n        xlabel=\"Epoch\",\n        ylabel=\"\",\n        ylim=(0, 1),\n    )\n\n    fig.tight_layout()\n    fig.subplots_adjust(top=0.8)\n\n    sns.move_legend(\n        obj=ax1, loc=\"upper center\", bbox_to_anchor=(1.1, 1.3), ncol=2, frameon=False\n    )\n    sns.despine()\n\n    plt.show()
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/models/pytorch_helpers.py b/models/pytorch_helpers.py
--- a/models/pytorch_helpers.py	(revision 64154792a8fdae873b2b1921c147664a35b5ff16)
+++ b/models/pytorch_helpers.py	(date 1641552492835)
@@ -1,6 +1,7 @@
 import time
 
 import matplotlib.pyplot as plt
+import matplotlib.ticker as mticker
 from matplotlib.ticker import ScalarFormatter
 import numpy as np
 import seaborn as sns
@@ -131,6 +132,7 @@
     num_epochs,
     train_dataloader,
     val_dataloader,
+    scheduler=None,
     save_best=False,
     save_path=None,
     verbose=False,
@@ -151,6 +153,10 @@
             loss_function=loss_function,
             device=device,
         )
+
+        if scheduler is not None:
+            scheduler.step()
+
         epoch_val_loss = validate_regression(
             dataloader=val_dataloader,
             model=model,
@@ -195,23 +201,26 @@
     return train_losses, val_losses
 
 
-def plot_regression(train_losses, val_losses):
+def plot_regression(train_losses, val_losses, lr):
     sns.set_theme(style="whitegrid")
 
     fig, ax = plt.subplots(figsize=(10, 5))
 
     epochs = range(1, len(train_losses) + 1)
 
-    ax.plot(epochs, train_losses, label="Training", marker="o")
-    ax.plot(epochs, val_losses, label="Validation", marker="o")
+    ax.plot(epochs, train_losses, label="Training") #, marker="o")
+    ax.plot(epochs, val_losses, label="Validation") #, marker="o")
     ax.set(
         title="Loss",
+        suptitle=f"Learning rate = {lr}",
         xlabel="Epoch",
         ylabel="",
     )
     ax.legend()
-    # ax.ticklabel_format(useOffset=False)
-    # ax.yaxis.set_major_formatter(ScalarFormatter(useOffset=False)) # tried to delete the 1e* on top of the plot
+
+    ticks_loc = ax.get_yticks().tolist()
+    ax.yaxis.set_major_locator(mticker.FixedLocator(ticks_loc))
+    ax.set_yticklabels(['{:,}'.format(int(x)) for x in ticks_loc])
 
     sns.despine()
     plt.show()
Index: models/metric_NN.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>#%%\nimport fastprogress\nimport os\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom sklearn.model_selection import train_test_split\nfrom torch.utils.data import DataLoader, TensorDataset\n\nfrom pytorch_helpers import (\n    plot_regression,\n    print_data_shapes,\n    print_param_shapes,\n    run_regression,\n    generate_train_val_data_split,\n    init_data_loaders,\n)\nfrom sklearn_helpers import get_column_transformer\n\nos.chdir(\"/Users/marei/airbnb-oslo\")\n\n#%%\n# SECTION: PyTorch Training Test\nlistings_subset = pd.read_pickle(\"data-clean/listings_subset.pkl\")\n\nX = listings_subset.drop(columns=\"price\")\ny = listings_subset[\"price\"]\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=123, shuffle=True\n)\n\ncolumn_transformer = get_column_transformer()\n\nX_train_tensor = torch.tensor(\n    column_transformer.fit_transform(X_train).astype(np.float32)\n)\ny_train_tensor = torch.tensor(y_train.values.astype(np.float32))\ntrainset = TensorDataset(X_train_tensor, y_train_tensor)\n\nX_test_tensor = torch.tensor(column_transformer.transform(X_test).astype(np.float32))\ny_test_tensor = torch.tensor(y_test.values.astype(np.float32))\ntestset = TensorDataset(X_test_tensor, y_test_tensor)\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n#%%\n# BOOKMARK: Hyperparameters\nbatch_size = 64\n\n# NN structure\nin_features = len(trainset[0][0])\nhidden_features_list = [32, 64, 128, 64, 32]\ndropout_prob = 0.25\nloss_function = nn.MSELoss(reduction=\"sum\")\n\n#%%\n# BOOKMARK: Generate train-val split\n\ntrainset, valset = generate_train_val_data_split(trainset, split_seed=42, val_frac=0.2)\n\n#%%\n# BOOKMARK: DataLoaders\n\ntrainloader, valloader, testloader = init_data_loaders(trainset, valset, testset, batch_size)\n\n#%%\n# SECTION: Model Construction\nclass NN(nn.Module):\n    def __init__(self, in_features, hidden_features_list, dropout_prob):\n        super(NN, self).__init__()\n\n        self.input_layer = nn.Sequential(\n            nn.Linear(in_features, hidden_features_list[0], bias=True),\n            #nn.BatchNorm1d(hidden_features_list[0]),\n            nn.ReLU(),\n            #nn.Dropout(dropout_prob),\n        )\n\n        self.hidden_layers = self.hidden_block(\n            in_features=hidden_features_list[0],\n            out_features_list=hidden_features_list[1:],\n            dropout_prob=dropout_prob,\n        )\n\n        self.output_layer = nn.Linear(\n            in_features=hidden_features_list[-1], out_features=1\n        )\n\n    def forward(self, x):\n        x = self.input_layer(x)\n        x = self.hidden_layers(x)\n        x = self.output_layer(x)\n        return x\n\n    def hidden_block(self, in_features, out_features_list, dropout_prob):\n        layers = []\n        for out_features in out_features_list:\n            layers.append(nn.Linear(in_features, out_features, bias=True))\n            #layers.append(nn.BatchNorm1d(out_features))\n            layers.append(nn.ReLU())\n            #layers.append(nn.Dropout(p=dropout_prob))\n            in_features = out_features\n\n        return nn.Sequential(*layers)\n\n\nmodel = NN(in_features, hidden_features_list, dropout_prob).to(device)\n\n#%%\nprint(\"Parameter shapes\")\nprint_param_shapes(model)\n\n#%%\n# SECTION: Model Training\n#model = NN(in_features, hidden_features_list, dropout_prob).to(device)\n\nnum_epochs = 50\nlr = 0.01\noptimizer = optim.Adam(params=model.parameters(), lr=lr)\n\ntrain_losses, val_losses = run_regression(\n    model,\n    optimizer,\n    loss_function,\n    device,\n    num_epochs,\n    trainloader,\n    testloader,\n    verbose=True,\n    save_best=True,\n)\n\nplot_regression(train_losses, val_losses)\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/models/metric_NN.py b/models/metric_NN.py
--- a/models/metric_NN.py	(revision 64154792a8fdae873b2b1921c147664a35b5ff16)
+++ b/models/metric_NN.py	(date 1641547686619)
@@ -6,6 +6,7 @@
 import torch
 import torch.nn as nn
 import torch.optim as optim
+from torch.optim.lr_scheduler import StepLR
 from sklearn.model_selection import train_test_split
 from torch.utils.data import DataLoader, TensorDataset
 
@@ -52,6 +53,7 @@
 
 # NN structure
 in_features = len(trainset[0][0])
+# hidden_features_list = [32, 64, 128, 64, 32]
 hidden_features_list = [32, 64, 128, 64, 32]
 dropout_prob = 0.25
 loss_function = nn.MSELoss(reduction="sum")
@@ -74,7 +76,7 @@
 
         self.input_layer = nn.Sequential(
             nn.Linear(in_features, hidden_features_list[0], bias=True),
-            #nn.BatchNorm1d(hidden_features_list[0]),
+            # nn.BatchNorm1d(hidden_features_list[0]),
             nn.ReLU(),
             #nn.Dropout(dropout_prob),
         )
@@ -99,7 +101,7 @@
         layers = []
         for out_features in out_features_list:
             layers.append(nn.Linear(in_features, out_features, bias=True))
-            #layers.append(nn.BatchNorm1d(out_features))
+            # layers.append(nn.BatchNorm1d(out_features))
             layers.append(nn.ReLU())
             #layers.append(nn.Dropout(p=dropout_prob))
             in_features = out_features
@@ -117,9 +119,10 @@
 # SECTION: Model Training
 #model = NN(in_features, hidden_features_list, dropout_prob).to(device)
 
-num_epochs = 50
-lr = 0.01
+num_epochs = 2000
+lr = 0.0001
 optimizer = optim.Adam(params=model.parameters(), lr=lr)
+#scheduler = StepLR(optimizer, step_size=100)
 
 train_losses, val_losses = run_regression(
     model,
@@ -129,8 +132,9 @@
     num_epochs,
     trainloader,
     testloader,
+    # scheduler,
     verbose=True,
     save_best=True,
 )
 
-plot_regression(train_losses, val_losses)
+plot_regression(train_losses, val_losses, lr)
